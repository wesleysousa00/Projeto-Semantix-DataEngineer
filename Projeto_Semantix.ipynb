{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Final Curso Data Engineer - Semantix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aluno: Wesley Sousa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- O objetivo deste projeto é desmontrar as habilidades adquiridas neste curso, realizando um projeto prático utilizando diferentes ferramentas de Big Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enviar os dados para o hdfs\n",
    "\n",
    "- Dados: https://mobileapps.saude.gov.br/esus-vepi/files/unAFkcaNDeXajurGB7LChj8SgQYS2ptm/04bd3419b22b9cc5c6efac2c6528100d_HIST_PAINEL_COVIDBR_06jul2021.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiro passo é criar um diretório chamado covid no hdfs\n",
    "\n",
    "!hdfs dfs -mkdir /user/wesley/covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 01:40 /user/wesley/covid\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-01 13:38 /user/wesley/data\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-07-29 11:12 /user/wesley/names_us_parquet\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-02 11:56 /user/wesley/projeto_python\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-01 14:31 /user/wesley/relatorio_anual\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-03 11:50 /user/wesley/stream\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-01 13:38 /user/wesley/teste_csv\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 19:48 /user/wesley/visual2\r\n"
     ]
    }
   ],
   "source": [
    "# Verificando se o diretório foi criado\n",
    "\n",
    "!hdfs dfs -ls /user/wesley/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Baixei o arquivo no Linux na seguinte pasta docker-bigdata/input/exercises-data\n",
    "- sudo curl -O https://mobileapps.saude.gov.br/esus-vepi/files/unAFkcaNDeXajurGB7LChj8SgQYS2ptm/04bd3419b22b9cc5c6efac2c6528100d_HIST_PAINEL_COVIDBR_06jul2021.rar\n",
    "- Após baixar o arquivo, foi necessário baixar um descompactador, para isto utilizei os comandos abaixo.\n",
    "- sudo apt-get install rar unrar\n",
    "- e o comando para descompactar o arquivo\n",
    "- sudo unrar x 04bd3419b22b9cc5c6efac2c6528100d_HIST_PAINEL_COVIDBR_06jul2021.rar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para enviar o arquivo para o hdfs, precisamos acessar o container do namenode utilizando o comando\n",
    "- docker exec -it namenode bash ou docker exec -it namenode /bin/bash\n",
    "- Depois de acessar, precisamos enviar os arquivos utilizando os comandos abaixo\n",
    "- root@namenode:/# hdfs dfs -put /input/exercises-data/HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv /user/wesley/covid\n",
    "- root@namenode:/# hdfs dfs -put /input/exercises-data/HIST_PAINEL_COVIDBR_2020_Parte2_06jul2021.csv /user/wesley/covid\n",
    "- root@namenode:/# hdfs dfs -put /input/exercises-data/HIST_PAINEL_COVIDBR_2021_Parte1_06jul2021.csv /user/wesley/covid\n",
    "- root@namenode:/# hdfs dfs -put /input/exercises-data/HIST_PAINEL_COVIDBR_2021_Parte2_06jul2021.csv /user/wesley/covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   3 root supergroup   62492959 2022-08-06 01:32 /user/wesley/covid/HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv\r\n",
      "-rw-r--r--   3 root supergroup   76520681 2022-08-06 01:39 /user/wesley/covid/HIST_PAINEL_COVIDBR_2020_Parte2_06jul2021.csv\r\n",
      "-rw-r--r--   3 root supergroup   91120916 2022-08-06 01:39 /user/wesley/covid/HIST_PAINEL_COVIDBR_2021_Parte1_06jul2021.csv\r\n",
      "-rw-r--r--   3 root supergroup    3046774 2022-08-06 01:40 /user/wesley/covid/HIST_PAINEL_COVIDBR_2021_Parte2_06jul2021.csv\r\n"
     ]
    }
   ],
   "source": [
    "# Verificando se os dados estão no hdfs\n",
    "\n",
    "!hdfs dfs -ls /user/wesley/covid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Otimizar todos os dados do hdfs para uma tabela Hive particionada por município"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30010;53001;DISTRITO FEDERAL;2020-07-22;30;3015268;87801;1725;1176;18;;;1\r",
      "\r\n",
      "Centro-Oeste;DF;Brasília;53;530010;53001;DISTRITO FEDERAL;2020-07-23;30;3015268;90023;2222;1218;42;;;1\r",
      "\r\n",
      "Centro-Oeste;DF;Brasília;53;530010;53001;DISTRITO FEDERAL;2020-07-24;30;3015268;92414;2391;1244;26;;;1\r",
      "\r\n",
      "Centro-Oeste;DF;Brasília;53;530010;53001;DISTRITO FEDERAL;2020-07-25;30;3015268;94187;1773;1275;31;;;1\r",
      "\r\n",
      "Centro-Oeste;DF;Brasília;53;530010;53001;DISTRITO FEDERAL;2020-07-26;31;3015268;96332;2145;1308;33;;;1\r",
      "\r\n",
      "Centro-Oeste;DF;Brasília;53;530010;53001;DISTRITO FEDERAL;2020-07-27;31;3015268;98480;2148;1339;31;;;1\r",
      "\r\n",
      "Centro-Oeste;DF;Brasília;53;530010;53001;DISTRITO FEDERAL;2020-07-28;31;3015268;100726;2246;1391;52;;;1\r",
      "\r\n",
      "Centro-Oeste;DF;Brasília;53;530010;53001;DISTRITO FEDERAL;2020-07-29;31;3015268;102342;1616;1419;28;;;1\r",
      "\r\n",
      "Centro-Oeste;DF;Brasília;53;530010;53001;DISTRITO FEDERAL;2020-07-30;31;3015268;104442;2100;1444;25;;;1\r",
      "\r\n",
      "Centro-Oeste;DF;Brasília;53;530010;53001;DISTRITO FEDERAL;2020-07-31;31;3015268;106292;1850;1469;25;;;1\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Verificando o conteúdo do arquivo\n",
    "\n",
    "!hdfs dfs -tail /user/wesley/covid/HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------+-----+------+--------------+---------------+-------------------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+\n",
      "|regiao|estado|municipio|coduf|codmun|codRegiaoSaude|nomeRegiaoSaude|               data|semanaEpi|populacaoTCU2019|casosAcumulado|casosNovos|obitosAcumulado|obitosNovos|Recuperadosnovos|emAcompanhamentoNovos|interior/metropolitana|\n",
      "+------+------+---------+-----+------+--------------+---------------+-------------------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-25 00:00:00|        9|       210147125|             0|         0|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-26 00:00:00|        9|       210147125|             1|         1|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-27 00:00:00|        9|       210147125|             1|         0|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-28 00:00:00|        9|       210147125|             1|         0|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-29 00:00:00|        9|       210147125|             2|         1|              0|          0|            null|                 null|                  null|\n",
      "+------+------+---------+-----+------+--------------+---------------+-------------------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lendo os arquivos para um dataframe\n",
    "\n",
    "df = spark.read.csv(\"/user/wesley/covid/*.csv\", sep=';', header=True, inferSchema = True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- regiao: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- coduf: integer (nullable = true)\n",
      " |-- codmun: integer (nullable = true)\n",
      " |-- codRegiaoSaude: integer (nullable = true)\n",
      " |-- nomeRegiaoSaude: string (nullable = true)\n",
      " |-- data: timestamp (nullable = true)\n",
      " |-- semanaEpi: integer (nullable = true)\n",
      " |-- populacaoTCU2019: integer (nullable = true)\n",
      " |-- casosAcumulado: decimal(10,0) (nullable = true)\n",
      " |-- casosNovos: integer (nullable = true)\n",
      " |-- obitosAcumulado: integer (nullable = true)\n",
      " |-- obitosNovos: integer (nullable = true)\n",
      " |-- Recuperadosnovos: integer (nullable = true)\n",
      " |-- emAcompanhamentoNovos: integer (nullable = true)\n",
      " |-- interior/metropolitana: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificando o schema da tabela\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------+----------------+---------------------+-------------------+\n",
      "|regiao|estado|municipio|Recuperadosnovos|emAcompanhamentoNovos|               data|\n",
      "+------+------+---------+----------------+---------------------+-------------------+\n",
      "|Brasil|  null|     null|            null|                 null|2020-02-25 00:00:00|\n",
      "|Brasil|  null|     null|            null|                 null|2020-02-26 00:00:00|\n",
      "|Brasil|  null|     null|            null|                 null|2020-02-27 00:00:00|\n",
      "|Brasil|  null|     null|            null|                 null|2020-02-28 00:00:00|\n",
      "|Brasil|  null|     null|            null|                 null|2020-02-29 00:00:00|\n",
      "|Brasil|  null|     null|            null|                 null|2020-03-01 00:00:00|\n",
      "|Brasil|  null|     null|            null|                 null|2020-03-02 00:00:00|\n",
      "|Brasil|  null|     null|            null|                 null|2020-03-03 00:00:00|\n",
      "|Brasil|  null|     null|            null|                 null|2020-03-04 00:00:00|\n",
      "|Brasil|  null|     null|            null|                 null|2020-03-05 00:00:00|\n",
      "+------+------+---------+----------------+---------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pegando apenas algumas colunas para entender o dataframe, com isto verifiquei que possui diversos valores nulos\n",
    "\n",
    "df.select(\"regiao\", \"estado\", \"municipio\", 'Recuperadosnovos', 'emAcompanhamentoNovos', \"data\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alterando o padrão de data, removendo a hora, e alterando o padrão para dia, mês ano\n",
    "\n",
    "df = df.withColumn('data', f.from_unixtime(f.unix_timestamp(df.data), \"dd-MM-yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------+----------------+---------------------+----------+\n",
      "|regiao|estado|municipio|Recuperadosnovos|emAcompanhamentoNovos|      data|\n",
      "+------+------+---------+----------------+---------------------+----------+\n",
      "|Brasil|  null|     null|            null|                 null|25-02-2020|\n",
      "|Brasil|  null|     null|            null|                 null|26-02-2020|\n",
      "|Brasil|  null|     null|            null|                 null|27-02-2020|\n",
      "|Brasil|  null|     null|            null|                 null|28-02-2020|\n",
      "|Brasil|  null|     null|            null|                 null|29-02-2020|\n",
      "|Brasil|  null|     null|            null|                 null|01-03-2020|\n",
      "|Brasil|  null|     null|            null|                 null|02-03-2020|\n",
      "|Brasil|  null|     null|            null|                 null|03-03-2020|\n",
      "|Brasil|  null|     null|            null|                 null|04-03-2020|\n",
      "|Brasil|  null|     null|            null|                 null|05-03-2020|\n",
      "+------+------+---------+----------------+---------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificando se foi alterado a data\n",
    "\n",
    "df.select(\"regiao\", \"estado\", \"municipio\", 'Recuperadosnovos', 'emAcompanhamentoNovos', \"data\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando o banco no Hive: \n",
    "- Para salvar os dados em uma tabela Hive, primeiro precisamos  acessar o Hive e criar um database\n",
    "- Acessando o Hive: docker exec -it hive-server bash\n",
    "- Acessando o Beeline: beeline -u jdbc:hive2://Localhost:10000\n",
    "- create database projeto_semantix comment \"banco de dados do para o projeto final do curso data enginner da semantix\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Salvando os dados em tabela hive particionado pelo municipio\n",
    "\n",
    "df.write.mode(\"overwrite\").partitionBy('municipio').saveAsTable('projeto_semantix.wesley')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirmando o particionamento por municipio\n",
    "# Não estarei executando essa célula, pois o particionamento é criado +5000 municipios, e isto faz com que esse jupyter notebook\n",
    "# Fiquei com +5000 linhas quando executado.\n",
    "\n",
    "!hdfs dfs -ls /user/hive/warehouse/projeto_semantix.db/wesley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Criar 3 vizualizações pelo Spark com os dados enviados para o HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------+\n",
      "|Recuperadosnovos|emAcompanhamentoNovos|\n",
      "+----------------+---------------------+\n",
      "|        17262646|              1065477|\n",
      "+----------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A primeira visualzização é o total de recuperados e o total em Acompanhamentos\n",
    "# Utilizando a última data que possui essas informações atualizadas\n",
    "\n",
    "visual1 = df.select(\"Recuperadosnovos\", \"emAcompanhamentoNovos\").where(df.data == '06-07-2021')\n",
    "visual1.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+----------+\n",
      "|casosAcumulado|casosNovos|incidência|\n",
      "+--------------+----------+----------+\n",
      "|      18855015|     62504|    8972.3|\n",
      "+--------------+----------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A segunda visualização é o total de casos confirmados Acumulado, Casos Novos e Incidência\n",
    "\n",
    "df1 = df.where(df.data == '06-07-2021')\n",
    "visual2 = df1.select(\"casosAcumulado\", \n",
    "                     \"casosNovos\", \n",
    "                     f.round(df1['casosAcumulado']/df1['populacaoTCU2019']*100000,1)\n",
    "                     .alias(\"incidência\"))\n",
    "visual2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------+-----------+\n",
      "|obitosAcumulado|obitosNovos|Letalidade|Mortalidade|\n",
      "+---------------+-----------+----------+-----------+\n",
      "|         526892|       1780|       2.8|      250.7|\n",
      "+---------------+-----------+----------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A terceira visualização é óbitos acumulados, casos novos, letalidade e mortalidade\n",
    "# A letalidade seria o valor de obitos acumulado dividido pelo número de casos acumulado\n",
    "\n",
    "visual3 = df1.select(\"obitosAcumulado\", \n",
    "                     \"obitosNovos\",\n",
    "                     f.round(df1['obitosAcumulado']/df1['casosAcumulado']*100,1)\n",
    "                     .alias(\"Letalidade\"),\n",
    "                     f.round(df1['obitosAcumulado']/df1['populacaoTCU2019']*100000,1)\n",
    "                     .alias(\"Mortalidade\"))\n",
    "visual3.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Salvar a primeira visualização como tabela Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual1.write.mode(\"overwrite\").saveAsTable(\"projeto_semantix.visual1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 02:55 /user/hive/warehouse/projeto_semantix.db/covid\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 19:42 /user/hive/warehouse/projeto_semantix.db/visual1\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-08 17:02 /user/hive/warehouse/projeto_semantix.db/wesley\r\n"
     ]
    }
   ],
   "source": [
    "# Verificando se foi salvo no Hive\n",
    "\n",
    "!hdfs dfs -ls /user/hive/warehouse/projeto_semantix.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Salvar a segunda visualização com formato parquet e compressão snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual2.write.parquet(\"/user/wesley/visual2\", compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   2 root supergroup          0 2022-08-06 19:48 /user/wesley/visual2/_SUCCESS\r\n",
      "-rw-r--r--   2 root supergroup        496 2022-08-06 19:48 /user/wesley/visual2/part-00000-f2cf59e9-9535-4e93-9534-041b80c4a120-c000.snappy.parquet\r\n",
      "-rw-r--r--   2 root supergroup      56169 2022-08-06 19:48 /user/wesley/visual2/part-00003-f2cf59e9-9535-4e93-9534-041b80c4a120-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "# Verificando se foi salvo\n",
    "\n",
    "!hdfs dfs -ls /user/wesley/visual2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Salvar a terceira visualização em um tópico no Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual3.selectExpr(\"to_json(struct(*)) AS value\")\\\n",
    "    .write\\\n",
    "    .format('kafka')\\\n",
    "    .option('kafka.bootstrap.servers', 'kafka:9092')\\\n",
    "    .option('topic', 'obitos_brasil')\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+-------------------------------------------------------------------------------------------------------+\n",
    "|value                                                                                                  |\n",
    "+-------------------------------------------------------------------------------------------------------+\n",
    "|{\"Região\":\"Brasil\",\"Óbitos_Acumulados\":526892,\"Óbitos_Novos\":1780,\"Letalidade\":2.8,\"Mortalidade\":250.7}|\n",
    "|{\"Região\":\"Brasil\",\"Óbitos_Acumulados\":526892,\"Óbitos_Novos\":1780,\"Letalidade\":2.8,\"Mortalidade\":250.7}|\n",
    "+-------------------------------------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo o arquivo que foi salvo no tópico do kafka\n",
    "\n",
    "obitos = spark.read.format('kafka').option('kafka.bootstrap.servers', 'kafka:9092').option('subscribe', 'visual3').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Criar a visualização pelo Spark com os dados enviados para o HDFS:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------+----------------------+-----------------------+\n",
      "|      regiao|   casos|Óbitos|Incidência/100mil hab.|Mortalidade/100mil hab.|\n",
      "+------------+--------+------+----------------------+-----------------------+\n",
      "|      Brasil|18855015|526892|                8972.3|                  250.7|\n",
      "|Centro-Oeste| 3833238| 98414|               54617.3|                 1402.2|\n",
      "|    Nordeste| 8911474|215648|               59916.9|                 1449.9|\n",
      "|       Norte| 3465630| 87690|               40284.6|                 1019.3|\n",
      "|     Sudeste|14277606|490622|               31093.0|                 1068.4|\n",
      "|         Sul| 7222082|161410|               63163.5|                 1411.7|\n",
      "+------------+--------+------+----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visual 4 mostra o total de Casos, Óbitos, Incidência/100mil hab, Mortalidade/100mil hab\n",
    "\n",
    "visual4 = df1.groupBy('regiao')\\\n",
    "            .agg(sum('casosAcumulado')\\\n",
    "            .alias(\"casos\"),sum('obitosAcumulado')\\\n",
    "            .alias('Óbitos'),max('populacaoTCU2019')\\\n",
    "            .alias(\"Populacao\"))\n",
    "\n",
    "visual4 = (visual4.withColumn('Incidência/100mil hab.', f.round(visual4['casos']/visual4['Populacao']*100000,1))\n",
    "          .withColumn('Mortalidade/100mil hab.', f.round(visual4['Óbitos']/visual4['Populacao']*100000,1)))\n",
    "\n",
    "visual4 = visual4.drop('Populacao')\n",
    "\n",
    "visual4.sort(col('regiao').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
